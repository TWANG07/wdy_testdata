[2025-11-18 19:54:26,638] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.6
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
[2025-11-18 19:54:36,797] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7: setting --include=localhost:0,1,2,3,4,5,6,7
[2025-11-18 19:54:36,797] [INFO] [runner.py:568:main] cmd = /usr/local/bin/python3 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None llava/train/train_mem.py --deepspeed ./scripts/zero2.json --model_name_or_path ./checkpoints/llava-v1.6-vicuna-13b --version plain --data_path /home/llava/LLaVA/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json --image_folder /home/llava/LLaVA/data/LLaVA-Pretrain/images --vision_tower ./checkpoints/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --tune_mm_mlp_adapter True --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --bf16 True --output_dir ./log/llava-v1.6-vicuna-13b-pretrain --num_train_epochs 1 --per_device_train_batch_size 32 --per_device_eval_batch_size 4 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 24000 --save_total_limit 1 --learning_rate 1e-3 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True
[2025-11-18 19:54:40,147] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.6
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
[2025-11-18 19:54:50,022] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}
[2025-11-18 19:54:50,022] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0
[2025-11-18 19:54:50,022] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})
[2025-11-18 19:54:50,022] [INFO] [launch.py:164:main] dist_world_size=8
[2025-11-18 19:54:50,022] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7
[2025-11-18 19:54:50,023] [INFO] [launch.py:256:main] process 25828 spawned with command: ['/usr/local/bin/python3', '-u', 'llava/train/train_mem.py', '--local_rank=0', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', './checkpoints/llava-v1.6-vicuna-13b', '--version', 'plain', '--data_path', '/home/llava/LLaVA/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json', '--image_folder', '/home/llava/LLaVA/data/LLaVA-Pretrain/images', '--vision_tower', './checkpoints/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './log/llava-v1.6-vicuna-13b-pretrain', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '24000', '--save_total_limit', '1', '--learning_rate', '1e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True']
[2025-11-18 19:54:50,024] [INFO] [launch.py:256:main] process 25829 spawned with command: ['/usr/local/bin/python3', '-u', 'llava/train/train_mem.py', '--local_rank=1', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', './checkpoints/llava-v1.6-vicuna-13b', '--version', 'plain', '--data_path', '/home/llava/LLaVA/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json', '--image_folder', '/home/llava/LLaVA/data/LLaVA-Pretrain/images', '--vision_tower', './checkpoints/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './log/llava-v1.6-vicuna-13b-pretrain', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '24000', '--save_total_limit', '1', '--learning_rate', '1e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True']
[2025-11-18 19:54:50,025] [INFO] [launch.py:256:main] process 25830 spawned with command: ['/usr/local/bin/python3', '-u', 'llava/train/train_mem.py', '--local_rank=2', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', './checkpoints/llava-v1.6-vicuna-13b', '--version', 'plain', '--data_path', '/home/llava/LLaVA/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json', '--image_folder', '/home/llava/LLaVA/data/LLaVA-Pretrain/images', '--vision_tower', './checkpoints/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './log/llava-v1.6-vicuna-13b-pretrain', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '24000', '--save_total_limit', '1', '--learning_rate', '1e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True']
[2025-11-18 19:54:50,025] [INFO] [launch.py:256:main] process 25831 spawned with command: ['/usr/local/bin/python3', '-u', 'llava/train/train_mem.py', '--local_rank=3', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', './checkpoints/llava-v1.6-vicuna-13b', '--version', 'plain', '--data_path', '/home/llava/LLaVA/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json', '--image_folder', '/home/llava/LLaVA/data/LLaVA-Pretrain/images', '--vision_tower', './checkpoints/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './log/llava-v1.6-vicuna-13b-pretrain', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '24000', '--save_total_limit', '1', '--learning_rate', '1e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True']
[2025-11-18 19:54:50,026] [INFO] [launch.py:256:main] process 25832 spawned with command: ['/usr/local/bin/python3', '-u', 'llava/train/train_mem.py', '--local_rank=4', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', './checkpoints/llava-v1.6-vicuna-13b', '--version', 'plain', '--data_path', '/home/llava/LLaVA/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json', '--image_folder', '/home/llava/LLaVA/data/LLaVA-Pretrain/images', '--vision_tower', './checkpoints/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './log/llava-v1.6-vicuna-13b-pretrain', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '24000', '--save_total_limit', '1', '--learning_rate', '1e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True']
[2025-11-18 19:54:50,026] [INFO] [launch.py:256:main] process 25833 spawned with command: ['/usr/local/bin/python3', '-u', 'llava/train/train_mem.py', '--local_rank=5', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', './checkpoints/llava-v1.6-vicuna-13b', '--version', 'plain', '--data_path', '/home/llava/LLaVA/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json', '--image_folder', '/home/llava/LLaVA/data/LLaVA-Pretrain/images', '--vision_tower', './checkpoints/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './log/llava-v1.6-vicuna-13b-pretrain', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '24000', '--save_total_limit', '1', '--learning_rate', '1e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True']
[2025-11-18 19:54:50,027] [INFO] [launch.py:256:main] process 25834 spawned with command: ['/usr/local/bin/python3', '-u', 'llava/train/train_mem.py', '--local_rank=6', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', './checkpoints/llava-v1.6-vicuna-13b', '--version', 'plain', '--data_path', '/home/llava/LLaVA/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json', '--image_folder', '/home/llava/LLaVA/data/LLaVA-Pretrain/images', '--vision_tower', './checkpoints/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './log/llava-v1.6-vicuna-13b-pretrain', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '24000', '--save_total_limit', '1', '--learning_rate', '1e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True']
[2025-11-18 19:54:50,027] [INFO] [launch.py:256:main] process 25835 spawned with command: ['/usr/local/bin/python3', '-u', 'llava/train/train_mem.py', '--local_rank=7', '--deepspeed', './scripts/zero2.json', '--model_name_or_path', './checkpoints/llava-v1.6-vicuna-13b', '--version', 'plain', '--data_path', '/home/llava/LLaVA/data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json', '--image_folder', '/home/llava/LLaVA/data/LLaVA-Pretrain/images', '--vision_tower', './checkpoints/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--tune_mm_mlp_adapter', 'True', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--bf16', 'True', '--output_dir', './log/llava-v1.6-vicuna-13b-pretrain', '--num_train_epochs', '1', '--per_device_train_batch_size', '32', '--per_device_eval_batch_size', '4', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '24000', '--save_total_limit', '1', '--learning_rate', '1e-3', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True']
[2025-11-18 19:55:04,388] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-11-18 19:55:04,433] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-11-18 19:55:04,467] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[2025-11-18 19:55:04,525] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.6
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.6
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-11-18 19:55:04,631] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.6
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
[2025-11-18 19:55:04,651] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.6
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[2025-11-18 19:55:04,723] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-11-18 19:55:04,803] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.6
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.6
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.6
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2025-11-18 19:55:04,983] [INFO] [comm.py:637:init_distributed] cdb=None
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.6
[93m [WARNING] [0m using untested triton version (3.1.0), only 1.0.0 is known to be compatible
[2025-11-18 19:55:05,035] [INFO] [comm.py:637:init_distributed] cdb=None
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
[2025-11-18 19:55:05,225] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-11-18 19:55:05,248] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-11-18 19:55:05,257] [INFO] [comm.py:637:init_distributed] cdb=None
[2025-11-18 19:55:05,257] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-11-18 19:55:05,298] [INFO] [comm.py:637:init_distributed] cdb=None
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
[2025-11-18 19:55:05,311] [INFO] [comm.py:637:init_distributed] cdb=None
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:47: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @autocast_custom_fwd
/usr/local/lib/python3.12/site-packages/deepspeed/runtime/zero/linear.py:66: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @autocast_custom_bwd
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
[2025-11-18 19:55:05,538] [INFO] [comm.py:637:init_distributed] cdb=None
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:00<00:03,  1.30it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:01<00:08,  1.61s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:01<00:08,  1.71s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 2/6 [00:02<00:05,  1.31s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:01<00:08,  1.79s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:01<00:08,  1.66s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:01<00:07,  1.58s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:01<00:08,  1.76s/it]Loading checkpoint shards:  17%|â–ˆâ–‹        | 1/6 [00:01<00:07,  1.53s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 2/6 [00:02<00:05,  1.31s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 2/6 [00:02<00:05,  1.40s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:03<00:03,  1.26s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 2/6 [00:02<00:05,  1.45s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 2/6 [00:02<00:05,  1.42s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 2/6 [00:03<00:05,  1.49s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 2/6 [00:03<00:06,  1.63s/it]Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 2/6 [00:02<00:05,  1.44s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:03<00:03,  1.12s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:03<00:03,  1.24s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:04<00:02,  1.15s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:04<00:04,  1.39s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:04<00:04,  1.47s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:04<00:02,  1.21s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:04<00:04,  1.54s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:04<00:04,  1.62s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 3/6 [00:04<00:04,  1.49s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:05<00:02,  1.33s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5/6 [00:06<00:01,  1.25s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:05<00:02,  1.22s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5/6 [00:06<00:01,  1.17s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:05<00:02,  1.37s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:05<00:02,  1.41s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:06<00:02,  1.45s/it]Loading checkpoint shards:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 4/6 [00:05<00:02,  1.40s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5/6 [00:06<00:01,  1.23s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5/6 [00:06<00:01,  1.10s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5/6 [00:06<00:01,  1.23s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.57s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.38s/it]
Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5/6 [00:07<00:01,  1.42s/it]Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5/6 [00:07<00:01,  1.45s/it]/home/llava/LLaVA/checkpoints/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
Loading checkpoint shards:  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 5/6 [00:07<00:01,  1.41s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.42s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.33s/it]
/home/llava/LLaVA/checkpoints/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.46s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.39s/it]
/home/llava/LLaVA/checkpoints/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.36s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:07<00:00,  1.33s/it]
/home/llava/LLaVA/checkpoints/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.41s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.40s/it]
/home/llava/LLaVA/checkpoints/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:09<00:00,  1.55s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:09<00:00,  1.55s/it]
/home/llava/LLaVA/checkpoints/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:09<00:00,  1.57s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:09<00:00,  1.53s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.56s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6/6 [00:08<00:00,  1.50s/it]
/home/llava/LLaVA/checkpoints/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
/home/llava/LLaVA/checkpoints/clip-vit-large-patch14-336 is already loaded, `load_model` called again, skipping.
/usr/local/lib/python3.12/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/usr/local/lib/python3.12/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/usr/local/lib/python3.12/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/usr/local/lib/python3.12/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/usr/local/lib/python3.12/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/usr/local/lib/python3.12/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
Formatting inputs...Skip in lazy mode
/usr/local/lib/python3.12/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
/usr/local/lib/python3.12/site-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: 
dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)
  warnings.warn(
  0%|          | 0/2181 [00:00<?, ?it/s]/usr/local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/usr/local/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  return fn(*args, **kwargs)
/usr/local/lib/python3.12/site-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  0%|          | 1/2181 [00:19<11:41:38, 19.31s/it]                                                   {'loss': 3.519, 'learning_rate': 1.5151515151515153e-05, 'epoch': 0.0}
  0%|          | 1/2181 [00:19<11:41:38, 19.31s/it]  0%|          | 2/2181 [00:33<9:51:00, 16.27s/it]                                                   {'loss': 3.4705, 'learning_rate': 3.0303030303030306e-05, 'epoch': 0.0}
  0%|          | 2/2181 [00:33<9:51:00, 16.27s/it]  0%|          | 3/2181 [00:47<9:18:07, 15.38s/it]                                                  {'loss': 3.561, 'learning_rate': 4.545454545454546e-05, 'epoch': 0.0}
  0%|          | 3/2181 [00:47<9:18:07, 15.38s/it]  0%|          | 4/2181 [01:02<9:02:30, 14.95s/it]                                                  {'loss': 3.1334, 'learning_rate': 6.060606060606061e-05, 'epoch': 0.0}
  0%|          | 4/2181 [01:02<9:02:30, 14.95s/it]  0%|          | 5/2181 [01:16<8:51:41, 14.66s/it]                                                  {'loss': 2.7091, 'learning_rate': 7.575757575757576e-05, 'epoch': 0.0}
  0%|          | 5/2181 [01:16<8:51:41, 14.66s/it]  0%|          | 6/2181 [01:30<8:44:50, 14.48s/it]                                                  {'loss': 2.6569, 'learning_rate': 9.090909090909092e-05, 'epoch': 0.0}
  0%|          | 6/2181 [01:30<8:44:50, 14.48s/it]  0%|          | 7/2181 [01:44<8:40:38, 14.37s/it]                                                  {'loss': 2.5073, 'learning_rate': 0.00010606060606060606, 'epoch': 0.0}
  0%|          | 7/2181 [01:44<8:40:38, 14.37s/it]  0%|          | 8/2181 [01:58<8:37:45, 14.30s/it]                                                  {'loss': 2.4876, 'learning_rate': 0.00012121212121212122, 'epoch': 0.0}
  0%|          | 8/2181 [01:58<8:37:45, 14.30s/it]  0%|          | 9/2181 [02:12<8:35:41, 14.25s/it]                                                  {'loss': 2.504, 'learning_rate': 0.00013636363636363637, 'epoch': 0.0}
  0%|          | 9/2181 [02:12<8:35:41, 14.25s/it]  0%|          | 10/2181 [02:26<8:34:15, 14.21s/it]                                                   {'loss': 2.4399, 'learning_rate': 0.00015151515151515152, 'epoch': 0.0}
  0%|          | 10/2181 [02:26<8:34:15, 14.21s/it]  1%|          | 11/2181 [02:41<8:33:03, 14.19s/it]                                                   {'loss': 2.512, 'learning_rate': 0.00016666666666666666, 'epoch': 0.01}
  1%|          | 11/2181 [02:41<8:33:03, 14.19s/it]  1%|          | 12/2181 [02:55<8:32:10, 14.17s/it]                                                   {'loss': 2.4901, 'learning_rate': 0.00018181818181818183, 'epoch': 0.01}
  1%|          | 12/2181 [02:55<8:32:10, 14.17s/it]  1%|          | 13/2181 [03:09<8:31:33, 14.16s/it]                                                   {'loss': 2.4678, 'learning_rate': 0.00019696969696969695, 'epoch': 0.01}
  1%|          | 13/2181 [03:09<8:31:33, 14.16s/it]  1%|          | 14/2181 [03:23<8:31:08, 14.15s/it]                                                   {'loss': 2.4547, 'learning_rate': 0.00021212121212121213, 'epoch': 0.01}
  1%|          | 14/2181 [03:23<8:31:08, 14.15s/it]  1%|          | 15/2181 [03:37<8:30:41, 14.15s/it]                                                   {'loss': 2.4227, 'learning_rate': 0.00022727272727272727, 'epoch': 0.01}
  1%|          | 15/2181 [03:37<8:30:41, 14.15s/it]  1%|          | 16/2181 [03:51<8:32:06, 14.19s/it]                                                   {'loss': 2.4383, 'learning_rate': 0.00024242424242424245, 'epoch': 0.01}
  1%|          | 16/2181 [03:51<8:32:06, 14.19s/it]  1%|          | 17/2181 [04:05<8:31:13, 14.17s/it]                                                   {'loss': 2.3635, 'learning_rate': 0.00025757575757575756, 'epoch': 0.01}
  1%|          | 17/2181 [04:05<8:31:13, 14.17s/it]  1%|          | 18/2181 [04:20<8:30:34, 14.16s/it]                                                   {'loss': 2.3171, 'learning_rate': 0.00027272727272727274, 'epoch': 0.01}
  1%|          | 18/2181 [04:20<8:30:34, 14.16s/it]  1%|          | 19/2181 [04:34<8:29:58, 14.15s/it]                                                   {'loss': 2.3598, 'learning_rate': 0.0002878787878787879, 'epoch': 0.01}
  1%|          | 19/2181 [04:34<8:29:58, 14.15s/it]  1%|          | 20/2181 [04:48<8:31:19, 14.20s/it]                                                   {'loss': 2.26, 'learning_rate': 0.00030303030303030303, 'epoch': 0.01}
  1%|          | 20/2181 [04:48<8:31:19, 14.20s/it]  1%|          | 21/2181 [05:02<8:30:28, 14.18s/it]                                                   {'loss': 2.3752, 'learning_rate': 0.0003181818181818182, 'epoch': 0.01}
  1%|          | 21/2181 [05:02<8:30:28, 14.18s/it]  1%|          | 22/2181 [05:17<8:35:10, 14.32s/it]                                                   {'loss': 2.2247, 'learning_rate': 0.0003333333333333333, 'epoch': 0.01}
  1%|          | 22/2181 [05:17<8:35:10, 14.32s/it]  1%|          | 23/2181 [05:31<8:32:57, 14.26s/it]                                                   {'loss': 2.1993, 'learning_rate': 0.0003484848484848485, 'epoch': 0.01}
  1%|          | 23/2181 [05:31<8:32:57, 14.26s/it]  1%|          | 24/2181 [05:45<8:31:16, 14.22s/it]                                                   {'loss': 2.3776, 'learning_rate': 0.00036363636363636367, 'epoch': 0.01}
  1%|          | 24/2181 [05:45<8:31:16, 14.22s/it]  1%|          | 25/2181 [05:59<8:30:04, 14.19s/it]                                                   {'loss': 2.2063, 'learning_rate': 0.0003787878787878788, 'epoch': 0.01}
  1%|          | 25/2181 [05:59<8:30:04, 14.19s/it]  1%|          | 26/2181 [06:13<8:27:25, 14.13s/it]                                                   {'loss': 2.1613, 'learning_rate': 0.0003939393939393939, 'epoch': 0.01}
  1%|          | 26/2181 [06:13<8:27:25, 14.13s/it]  1%|          | 27/2181 [06:27<8:27:17, 14.13s/it]                                                   {'loss': 2.3117, 'learning_rate': 0.00040909090909090913, 'epoch': 0.01}
  1%|          | 27/2181 [06:27<8:27:17, 14.13s/it]  1%|â–         | 28/2181 [06:41<8:27:04, 14.13s/it]                                                   {'loss': 2.3361, 'learning_rate': 0.00042424242424242425, 'epoch': 0.01}
  1%|â–         | 28/2181 [06:41<8:27:04, 14.13s/it]  1%|â–         | 29/2181 [06:56<8:26:54, 14.13s/it]                                                   {'loss': 2.1767, 'learning_rate': 0.0004393939393939394, 'epoch': 0.01}
  1%|â–         | 29/2181 [06:56<8:26:54, 14.13s/it]  1%|â–         | 30/2181 [07:10<8:26:40, 14.13s/it]                                                   {'loss': 2.2607, 'learning_rate': 0.00045454545454545455, 'epoch': 0.01}
  1%|â–         | 30/2181 [07:10<8:26:40, 14.13s/it]  1%|â–         | 31/2181 [07:24<8:26:32, 14.14s/it]                                                   {'loss': 2.1768, 'learning_rate': 0.0004696969696969697, 'epoch': 0.01}
  1%|â–         | 31/2181 [07:24<8:26:32, 14.14s/it]  1%|â–         | 32/2181 [07:38<8:26:15, 14.13s/it]                                                   {'loss': 2.1784, 'learning_rate': 0.0004848484848484849, 'epoch': 0.01}
  1%|â–         | 32/2181 [07:38<8:26:15, 14.13s/it]  2%|â–         | 33/2181 [07:52<8:26:01, 14.13s/it]                                                   {'loss': 2.148, 'learning_rate': 0.0005, 'epoch': 0.02}
  2%|â–         | 33/2181 [07:52<8:26:01, 14.13s/it]  2%|â–         | 34/2181 [08:06<8:25:43, 14.13s/it]                                                   {'loss': 2.2883, 'learning_rate': 0.0005151515151515151, 'epoch': 0.02}
  2%|â–         | 34/2181 [08:06<8:25:43, 14.13s/it]  2%|â–         | 35/2181 [08:20<8:25:31, 14.13s/it]                                                   {'loss': 2.1706, 'learning_rate': 0.0005303030303030302, 'epoch': 0.02}
  2%|â–         | 35/2181 [08:20<8:25:31, 14.13s/it]  2%|â–         | 36/2181 [08:35<8:25:14, 14.13s/it]                                                   {'loss': 2.1531, 'learning_rate': 0.0005454545454545455, 'epoch': 0.02}
  2%|â–         | 36/2181 [08:35<8:25:14, 14.13s/it]  2%|â–         | 37/2181 [08:49<8:23:15, 14.08s/it]                                                   {'loss': 2.2552, 'learning_rate': 0.0005606060606060606, 'epoch': 0.02}
  2%|â–         | 37/2181 [08:49<8:23:15, 14.08s/it]  2%|â–         | 38/2181 [09:03<8:23:32, 14.10s/it]                                                   {'loss': 2.1475, 'learning_rate': 0.0005757575757575758, 'epoch': 0.02}
  2%|â–         | 38/2181 [09:03<8:23:32, 14.10s/it]  2%|â–         | 39/2181 [09:17<8:23:42, 14.11s/it]                                                   {'loss': 2.2395, 'learning_rate': 0.0005909090909090909, 'epoch': 0.02}
  2%|â–         | 39/2181 [09:17<8:23:42, 14.11s/it]  2%|â–         | 40/2181 [09:31<8:23:46, 14.12s/it]                                                   {'loss': 2.1499, 'learning_rate': 0.0006060606060606061, 'epoch': 0.02}
  2%|â–         | 40/2181 [09:31<8:23:46, 14.12s/it]  2%|â–         | 41/2181 [09:45<8:23:44, 14.12s/it]                                                   {'loss': 2.08, 'learning_rate': 0.0006212121212121212, 'epoch': 0.02}
  2%|â–         | 41/2181 [09:45<8:23:44, 14.12s/it]  2%|â–         | 42/2181 [09:59<8:23:33, 14.13s/it]                                                   {'loss': 2.1739, 'learning_rate': 0.0006363636363636364, 'epoch': 0.02}
  2%|â–         | 42/2181 [09:59<8:23:33, 14.13s/it]